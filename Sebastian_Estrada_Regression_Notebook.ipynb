{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/2q7j9g/E2hvfDNu7Mmm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sael17/Project_2_Notebooks_AISP23/blob/master/Sebastian_Estrada_Regression_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s6yREam9Qx_"
      },
      "outputs": [],
      "source": [
        "# Student Name: Sebastian A. Estrada Lopez\n",
        "# Student Number: 802-19-3839"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries to be used and set up\n",
        "\n",
        "Some of the libraries that are going to be used in here are common ones like numpy, matplotlib and PyTorch 2.0"
      ],
      "metadata": {
        "id": "WngOpPNGA4Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n"
      ],
      "metadata": {
        "id": "9VJThfW8BNJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtaining the dataset\n",
        "\n",
        "The dataset to be used for this experiment is 'The Auto MPG' dataset, which is available from the UCI Machine Learning Repository. This dataset contains the fuel efficiency of the late-1970s and early 1980s automobiles and the models to be built need to predict the miler per galon fuel efficiency. To do this, the models will be provided with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight. The data set contains the following attributes:\n",
        "\n",
        "1. mpg: continuous\n",
        "2. cylinders: multi-valued discrete\n",
        "3. displacement: continuous\n",
        "4. horsepower: continuous\n",
        "5. weight: continuous\n",
        "6. acceleration: continuous\n",
        "7. model year: multi-valued discrete\n",
        "8. origin: multi-valued discrete\n",
        "9. car name: string (unique for each instance)\n",
        "\n",
        "In the raw dataset it can be observed that the Car Name column is not very useful for the experiment, since the Name of a car does not affect its MPG. In that case, the Car Name column is ignored/removed from the dataset to be used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VpRkDgps_RmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Origin/source of the dataset\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "# List that contains the names for the columns for the dataset \n",
        "columns_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']\n",
        "# load the data using pandas\n",
        "raw_dataset = pd.read_csv(url,names=columns_names,na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "\n",
        "# make a copy of the original dataset in order to avoid risk of accidental manipulation\n",
        "raw_dataset_copy = raw_dataset.copy()\n",
        "# get the last 5 rows of the dataset\n",
        "raw_dataset_copy.tail()\n",
        "# display(raw_dataset[:50])"
      ],
      "metadata": {
        "id": "NYWcMWLdE2qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Data\n",
        "\n",
        "Before doing anything with the experiment, first we must understand what we are predicting and what our data is. We want to predict the Full Efficiency based on the other features that our data set contains. In the following sections, we will analyze and understand the data better"
      ],
      "metadata": {
        "id": "3saGIu0vZd5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis\n",
        "\n",
        "Before starting with the experiment, the data has to be cleaned as much as possible in order to reduce the risk of errors that can affect the analyis. "
      ],
      "metadata": {
        "id": "_kX6KaWuDalc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is sorted by Model year. Its is important that each batch is a random example of the data, and we do not want the model to make predictions based in the sorting of the model year. Therefore, the data is going to be shuffled to remove this sorting."
      ],
      "metadata": {
        "id": "zHlCnZtG3cjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to shuffle the order of the rows without touching the columns.\n",
        "# First, we get a list of indices corresponding to the rows.\n",
        "indices = np.arange(raw_dataset_copy.shape[0])\n",
        "print('Indices',indices, '\\n')\n",
        "\n",
        "# Next, we shuffle the indices using np.random.permutation but set a random seed\n",
        "# so that results can be replicated\n",
        "np.random.seed(0)\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "print('shuffled indices:', shuffled_indices, '\\n')\n",
        "\n",
        "# Finally, we use dataframe.reindex to change the ordering of the original\n",
        "# dataframe.\n",
        "raw_dataset_copy = raw_dataset_copy.reindex(shuffled_indices)\n",
        "display(raw_dataset_copy)"
      ],
      "metadata": {
        "id": "Fslq6wmg3y75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to clean the data by removing from it everything that is not a number"
      ],
      "metadata": {
        "id": "eEmCCbzr4n_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which columns of the dataset are not a number. We want to drop everything that is not a number in order to have clean numeric data\n",
        "raw_dataset_copy.isna().sum()\n",
        "# The dropna function drops rows with missing value(s) by default.\n",
        "raw_dataset_copy = raw_dataset_copy.dropna()"
      ],
      "metadata": {
        "id": "bqzI9H28Frz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"Origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies."
      ],
      "metadata": {
        "id": "-swSipxhItI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Associate each Origin ID with a specific country\n",
        "raw_dataset_copy['Origin'] = raw_dataset_copy['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
        "# Associate each Origin ID now with a one hot vector\n",
        "raw_dataset_copy = pd.get_dummies(raw_dataset_copy, columns=['Origin'], prefix='', prefix_sep='')\n",
        "# Display the last 5 rows to verify the dataset\n",
        "raw_dataset_copy.tail()                                        "
      ],
      "metadata": {
        "id": "R-oeW227IwSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the dataset has been cleaned, it will officially be the dataset for the experiment\n",
        "dataset = raw_dataset_copy\n",
        "# Free the space in memory from the old variable\n",
        "del raw_dataset_copy\n",
        "display(dataset)"
      ],
      "metadata": {
        "id": "8GHRBCPGKoBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test Split\n",
        "The data has to be split into Training and Test portions since the model will learn with the training data and then it will be evaluated with the test data to verify its performance. In this experiment, the split will be 80/20, this means that 80% will be training and the other 20% will be testing."
      ],
      "metadata": {
        "id": "laLUGdiRLg2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The input features to be used\n",
        "features = ['Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Europe', 'Japan', 'USA']\n",
        "# Use a ~80/20 data split\n",
        "X_train = dataset[:314]\n",
        "X_test = dataset[314:]\n",
        "\n",
        "## Create separate variables for features (inputs) and labels (outputs).\n",
        "# We will be using these in the cells below.\n",
        "dataset_train_features = X_train[features]\n",
        "dataset_test_features = X_test[features]\n",
        "dataset_train_labels = X_train['MPG']\n",
        "dataset_test_labels = X_test['MPG']\n",
        "\n",
        "\n",
        "# Confirm the data shapes are as expected.\n",
        "print('train data shape:', dataset_train_features.shape)\n",
        "print('train labels shape:', dataset_train_labels.shape)\n",
        "print('test data shape:', dataset_test_features.shape)\n",
        "print('test labels shape:', dataset_test_labels.shape)"
      ],
      "metadata": {
        "id": "MdpMe9FjMnL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline\n",
        "\n",
        "Before implementing the model for the experiment, we need to implement a basic baseline for it. In this case since we want to predict the fuel efficiency for a car, we will use the average price of the cars to predict regardless of the input. For the baseline evaluation we will be using Root Mean Square Error (RMSE) which is just the root square of Mean Square Error (MSE)."
      ],
      "metadata": {
        "id": "gI2vEVFn51jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline(Y):\n",
        "  # predict the average of the\n",
        "  return np.mean(Y)\n",
        "\n",
        "def RMSE(true_values, predicted_values):\n",
        "  MSE = (1/len(true_values)) * np.sum(np.power(np.subtract(true_values,predicted_values),2))\n",
        "  squared_error = np.sqrt(MSE)\n",
        "  return squared_error\n",
        "\n",
        "evaluated_baseline = baseline(dataset_train_labels)\n",
        "print(\"Evaluated Baseline:\",evaluated_baseline)\n",
        "\n",
        "print('RMSE Evaluated in the Training Data', RMSE(dataset_train_labels,evaluated_baseline))\n",
        "print('RMSE Evaluated in the Test Data', RMSE(dataset_test_labels,evaluated_baseline))\n"
      ],
      "metadata": {
        "id": "wiJN9_lA51SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Histograms\n",
        "Histograms can be used to interprate the data in a more visual way. Plotting histograms is s good way to starting building intuition about the data. This gives us a sense of the distribution of the data of each feature, but not how all features relate to each other."
      ],
      "metadata": {
        "id": "OyIFPXPLaAje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,3))\n",
        "for i in range(len(features)):\n",
        "  plt.subplot(1,9,i+1)\n",
        "  plt.hist(np.array(X_train[features[i]]))\n",
        "  plt.title(features[i])\n",
        "plt.show()\n",
        "\n",
        "display(X_train.describe())"
      ],
      "metadata": {
        "id": "RA_O3K4Ab-XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x0aiJBbKkTl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Correlations\n",
        "Lets check if there are correlations between the features in order to understand the data better. To determine if there is a correlation between two features, we can look at the value in the corresponding cell of the correlation matrix. The value in each cell represents the correlation coefficient between the two variables, which can range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 indicating no correlation.\n",
        "\n",
        "If the value is close to -1 or 1, it suggests a strong correlation between the two features. \n",
        "\n",
        "A Heatmap is going to be used in order to visualize represent the correlations between features. The resulting heatmap will have a color scale that represents the strength of the correlation between the features, with lighter colors indicating stronger positive correlations, darker colors indicating stronger negative correlations, and white indicating no correlation."
      ],
      "metadata": {
        "id": "PQAIEs58kn1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_correlations = pd.DataFrame(X_train)\n",
        "display(features_correlations.corr())\n",
        "\n",
        "sns.heatmap(features_correlations.corr(), cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "4QOoILE1k022"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diagonal of the heatmap is the strong relation between a feature and itself. Using the dark blue square in the top left of the heatmap, it can be analyzed that there is a strong correlation between the following features: Cylinders and displacement,Cylinders and horsepower, and cylinders and weight for example. \n",
        "\n",
        "However, we want to focus in our goal, in what we want to predict: MPG\n",
        "\n",
        "From the heatmap and the table above, it is observed the following:\n",
        "\n",
        "\n",
        "\n",
        "*   Negative Correlation between MPG and Cylinders. \n",
        "*   Negative Correlation between MPG and Displacement.\n",
        "*   Negative Correlation between MPG and Horsepower. \n",
        "*   Negative Correlation between MPG and Weight.\n",
        "*   Positive Correlation between MPG and Acceleration.\n",
        "*   Positive Correlation between MPG and Model Year.\n",
        "*   Positive Correlation between MPG and Europe.\n",
        "*   Positive Correlation between MPG and Japan.\n",
        "*   Negative Correlation between MPG and USA."
      ],
      "metadata": {
        "id": "nJwTFRVLnhnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Models"
      ],
      "metadata": {
        "id": "1edmtop2JMW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "\n",
        "For the models to be stable, the data has to be normalized. The data has to be normalized so that each feature is in the same scale. The STD and Mean will be used in order to normalize the data. Important: we can't normalize the test data by computing mean and variance on the test data, as this would violate our willful blindness of the test data."
      ],
      "metadata": {
        "id": "crUuMMANoZD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_normalization(data_frame_features,data_frame):\n",
        "  features_mean = data_frame_features.mean()\n",
        "  real_mean = data_frame-features_mean\n",
        "  standard_deviation = data_frame_features.std()\n",
        "  \n",
        "  return real_mean/standard_deviation\n",
        "\n",
        "\n",
        "train_features_norm = feature_normalization(dataset_train_features,dataset_train_features)\n",
        "test_features_norm = feature_normalization(dataset_train_features,dataset_test_features)\n",
        "\n",
        "\n",
        "print(\"Training Normalization\")\n",
        "display(train_features_norm.describe())\n",
        "print(\"Test Normalization\")\n",
        "display(test_features_norm.describe())"
      ],
      "metadata": {
        "id": "7Q2WV9eGH5YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Pandas DataFrames into Tensors.\n",
        "\n",
        "Before training the Models, we have to convert the Dataframes into Tensors so that we can pass the data to the models with PyTorch"
      ],
      "metadata": {
        "id": "EbEdAol7OMMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframes tensors\n",
        "tensor_data_train_features = torch.tensor(train_features_norm.values,dtype=torch.float32)\n",
        "tensor_data_train_labels = torch.tensor(dataset_train_labels.values,dtype=torch.float32).view(-1,1)\n",
        "tensor_data_test_features = torch.tensor(test_features_norm.values,dtype=torch.float32)\n",
        "tensor_data_test_labels = torch.tensor(dataset_test_labels.values,dtype=torch.float32).view(-1,1)"
      ],
      "metadata": {
        "id": "BdYZcSvADCb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Models \n",
        "\n",
        "All networks will use ReLU activation in the intermediate layers, but not in the final. Train for\n",
        "20 epochs with learning rate of 0.01.\n",
        "In the notebook add a cell and answer the following"
      ],
      "metadata": {
        "id": "6yTaNc4Fc3KK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Model #1\n",
        "\n",
        "For this experiment, we will be doing various regression models. This is due to the experiment requirements being predicting a number (MPG) based on other numbers (features). Model #1 will be a basic regression with 1 layer and 1 neuron"
      ],
      "metadata": {
        "id": "BpCCCSxZcY3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionModel(torch.nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super(RegressionModel,self).__init__()\n",
        "    # One Layer, One Neuron \n",
        "    self.layer1 = torch.nn.Linear(input_dim,1,bias=True)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_pred = self.layer1(x)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "iCUoErC6afUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Model #2\n",
        "\n",
        "Model #2 will be a neural network with the following characteristics:\n",
        "\n",
        "4-layer network\n",
        "\n",
        "\n",
        "1.   Layer 1 – 10 neurons\n",
        "2.   Layer 2 – 20 neurons\n",
        "3.   Layer 3 – 10 neurons\n",
        "4.   Layer 4 – output neuron"
      ],
      "metadata": {
        "id": "TEtEpsfEbtXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionModel_2(torch.nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super(RegressionModel_2,self).__init__()\n",
        "    # first layer receives the features as their inputs. This layer contains 10 neurons.\n",
        "    self.layer1 = torch.nn.Linear(input_dim,10,bias=True)\n",
        "    # second layer receives the 10 outputs from the prev layer. This layer contains 20 neurons.\n",
        "    self.layer2 = torch.nn.Linear(10,20,bias=True)\n",
        "    # Third layer receives the 20 outputs from the prev layer. This layer contains 10 neurons.\n",
        "    self.layer3 = torch.nn.Linear(20,10,bias=True)\n",
        "    # Forth layer receives the 10 outputs from the prev layer. This layer contains 1 neuron (the output).\n",
        "    self.layer4 = torch.nn.Linear(10,1,bias=True)\n",
        "    # activation function for the hidden layers\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "      y_pred = self.activation(self.layer1(x))\n",
        "      y_pred = self.activation(self.layer2(y_pred))\n",
        "      y_pred = self.activation(self.layer3(y_pred))\n",
        "      y_pred = self.layer4(y_pred)\n",
        "      return y_pred\n"
      ],
      "metadata": {
        "id": "hsREWiqLbi1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression Model #3\n",
        "\n",
        "Model #3 will be a neural network with the following characteristics:\n",
        "\n",
        "5-layer network\n",
        "\n",
        "\n",
        "1.   Layer 1 – 10 neurons\n",
        "2.   Layer 2 – 20 neurons\n",
        "3.   Layer 3 – 30 neurons\n",
        "4.   Layer 4 – 20 neurons\n",
        "5.   Layer 5 - output neuron"
      ],
      "metadata": {
        "id": "9aSPJawtdmqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionModel_3(torch.nn.Module):\n",
        "  def __init__(self,input_dim):\n",
        "    super(RegressionModel_3,self).__init__()\n",
        "    # first layer receives the features as their inputs. This layer contains 10 neurons.\n",
        "    self.layer1 = torch.nn.Linear(input_dim,10,bias=True)\n",
        "    # second layer receives the 10 outputs from the prev layer. This layer contains 20 neurons.\n",
        "    self.layer2 = torch.nn.Linear(10,20,bias=True)\n",
        "    # Third layer receives the 20 outputs from the prev layer. This layer contains 30 neurons.\n",
        "    self.layer3 = torch.nn.Linear(20,30,bias=True)\n",
        "    # Forth layer receives the 20 outputs from the prev layer. This layer contains 20 neurons.\n",
        "    self.layer4 = torch.nn.Linear(30,20,bias=True)\n",
        "    # Fifth layer receives the 20 outputs from the prev layer. This layer contains 1 neuron (the output).\n",
        "    self.layer5 = torch.nn.Linear(20,1,bias=True)\n",
        "    # activation function for the hidden layers\n",
        "    self.activation = torch.nn.ReLU()\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "      y_pred = self.activation(self.layer1(x))\n",
        "      y_pred = self.activation(self.layer2(y_pred))\n",
        "      y_pred = self.activation(self.layer3(y_pred))\n",
        "      y_pred = self.activation(self.layer4(y_pred))\n",
        "      y_pred = self.layer5(y_pred)\n",
        "      return y_pred"
      ],
      "metadata": {
        "id": "ue5TwJw7bmC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train models\n",
        "\n",
        "Now we have to train each model we have created. These will be our distinct experiments. The models will be trained with the training data previosly separated."
      ],
      "metadata": {
        "id": "GCZlC6IG7dko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The features will be the initial input for the models\n",
        "input_dim = len(features)\n",
        "# we just want to predict a number (MPG)\n",
        "output_dim = 1\n",
        "# Learning rate to be used\n",
        "lr = 0.01\n",
        "# number of iterations to be done \n",
        "epochs = 20\n",
        "# how much data will be passed to the model for each epoch\n",
        "batch_size = 32\n",
        "# how will the model evaluate its performance\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "tBnMhKpBc5hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and create a DataLoader object\n",
        "train_data = TensorDataset(tensor_data_train_features, tensor_data_train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(tensor_data_test_features, tensor_data_test_labels)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Y3yclSrCc36r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(neural_network_model_version):\n",
        "  '''\n",
        "     This function creates a Pytorch Neural Network, trains it and returns the loss produced\n",
        "     by the model\n",
        "     @params: integer representing which model the function will be created\n",
        "     @returns: the created and trained model and the loss list produced by the model \n",
        "     \n",
        "  '''\n",
        "\n",
        "  # These conditionals check which Regression Model version will be created\n",
        "  if neural_network_model_version == 1:\n",
        "    model = RegressionModel(input_dim)\n",
        "  elif neural_network_model_version == 2:\n",
        "    model = RegressionModel_2(input_dim)\n",
        "  else:\n",
        "    model = RegressionModel_3(input_dim)\n",
        "  # Depending on the model being created is the optimizer that the model will use\n",
        "  if neural_network_model_version==1:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  else:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  # Save the losses being produced so that it can be print out later\n",
        "  loss_list = []\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print statistics\n",
        "        print('{}, \\t{}, \\t{}'.format(epoch, loss.item(), [param.data for param in model.parameters()]))\n",
        "    # Append the loss to the loss list\n",
        "    loss_list.append(loss.item())\n",
        "  print('Finished Training')\n",
        "  return loss_list,model"
      ],
      "metadata": {
        "id": "YeNcT8pXf-jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model #1\n",
        "Having defined the settings for training our models, we proceed to train each of the models defined previously as classes above.\n",
        "Since this is model #1, we let know the training function that we want to build and train model #1. We save the trained model and the loss list produced."
      ],
      "metadata": {
        "id": "4jt3W_zzW_xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_loss_list,model_1 = train_model(neural_network_model_version=1)"
      ],
      "metadata": {
        "id": "Q7X_v0I-i3sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model #2\n",
        "Having defined the settings for training our models, we proceed to train each of the models defined previously as classes above.\n",
        "Since this is model #2, we let know the training function that we want to build and train model #2. We save the trained model and the loss list produced."
      ],
      "metadata": {
        "id": "6Z1gSDl2YOx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===================\")\n",
        "model_2_loss_list,model_2 = train_model(neural_network_model_version=2)"
      ],
      "metadata": {
        "id": "o03_-pdsFlEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Model #3\n",
        "Having defined the settings for training our models, we proceed to train each of the models defined previously as classes above.\n",
        "Since this is model #3, we let know the training function that we want to build and train model #3. We save the trained model and the loss list produced."
      ],
      "metadata": {
        "id": "3DW5t8_WYRcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===================\")\n",
        "model_3_loss_list,model_3 = train_model(neural_network_model_version=3)"
      ],
      "metadata": {
        "id": "4F677Zw7FlpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_3_loss_list)"
      ],
      "metadata": {
        "id": "rH3sAQMyeK1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Display Loss vs. Epochs for each of the models being evaluated\n",
        "For observation purposes, the loss of training for all models will be plotted so that the behavior of the models can be better analyzed."
      ],
      "metadata": {
        "id": "MaMSjIFy7tDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(model_1_loss_list, 'r',label='Model #1')\n",
        "plt.plot(model_2_loss_list, 'g',label='Model #2')\n",
        "plt.plot(model_3_loss_list, 'b',label='Model #3')\n",
        "plt.tight_layout()\n",
        "plt.grid('True', color='y')\n",
        "plt.xlabel(\"Epochs/Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wf0WEoAH2M2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Models\n",
        "Now, we want to see how our models behave with new data after being trained. This is the purpose of our test data."
      ],
      "metadata": {
        "id": "_AOsIK5N72U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model):\n",
        "  '''\n",
        "    Evaluate a model already trained with new/test data\n",
        "    @params: Model - This is the model that will be used to test the data on\n",
        "    @returns: A list containing the loss for the test evaluation\n",
        "  '''\n",
        "\n",
        "  # list to keep track of the test loss\n",
        "  loss_list = []\n",
        "  for epoch in range(epochs):\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "          inputs,labels = data\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs,labels)\n",
        "          # print diagnostic data\n",
        "          print('{}, \\t{}, \\t{}'.format(i, loss.item(), [param.data for param in model.parameters()]))      \n",
        "    loss_list.append(loss.item())\n",
        "  return loss_list\n"
      ],
      "metadata": {
        "id": "ptyT0lDCaAAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model #1 \n",
        "After training Model #1, we have to evaluate the model's performance with data that it has not seen before in order to analyze how good is at predicting our desired goal."
      ],
      "metadata": {
        "id": "fcVnIYZpZ45k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_test = test_model(model_1)"
      ],
      "metadata": {
        "id": "iPKhv9PEaHbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1_test)"
      ],
      "metadata": {
        "id": "4NE04zYddAN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model #2 \n",
        "After training Model #2, we have to evaluate the model's performance with data that it has not seen before in order to analyze how good is at predicting our desired goal."
      ],
      "metadata": {
        "id": "b6fFUsSiaKB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_test = test_model(model_2)"
      ],
      "metadata": {
        "id": "vqedu-mkaL4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Model #3 \n",
        "After training Model #3, we have to evaluate the model's performance with data that it has not seen before in order to analyze how good is at predicting our desired goal."
      ],
      "metadata": {
        "id": "nIrsp5xlaNbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_test = test_model(model_3)"
      ],
      "metadata": {
        "id": "UjqxuXWKaOJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Training and Test Loss \n",
        "\n",
        "In this section, we will be plotting the loss for each model and for both the training and testing as the epochs go. This is to analyze the behavior of the loss for both the training and loss data"
      ],
      "metadata": {
        "id": "jUG0wrgrUkb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(model_1_loss_list, 'r',label='Model #1')\n",
        "plt.plot(model_2_loss_list, 'g',label='Model #2')\n",
        "plt.plot(model_3_loss_list, 'b',label='Model #3')\n",
        "plt.plot(model_1_test, linestyle='--', color='r',label='Model #1 Test')\n",
        "plt.plot(model_2_test, linestyle='--', color='g',label='Model #2 Test')\n",
        "plt.plot(model_3_test, linestyle='--', color='b',label='Model #3 Test')\n",
        "plt.tight_layout()\n",
        "plt.grid('True', color='y')\n",
        "plt.xlabel(\"Epochs/Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "StZSu5OIblap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which of the three models had the least amount of error validation?\n",
        "\n",
        "Model #3 (Blue) has the least amount of error in validation. It can be observed that in the training loss, its the model that converges faster. Also, when looking at its loss in validation, it has the lowest validation compared with the other 3 models. An example of this can be looked when observing the values of the loss found in the both the training and test arrays for each model.\n"
      ],
      "metadata": {
        "id": "Oo00eVusB0gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the training loss for the 3 models\n",
        "print(\"Model #1 Training Loss\",model_1_loss_list)\n",
        "print(\"===================\")\n",
        "print(\"Model #2 Training Loss\",model_2_loss_list)\n",
        "print(\"===================\")\n",
        "print(\"Model #3 Training Loss\",model_3_loss_list)"
      ],
      "metadata": {
        "id": "52r-1Y1napon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the test loss for the 3 models\n",
        "print(\"Model #1 Test Loss\", model_1_test)\n",
        "print(\"===================\")\n",
        "print(\"Model #1 Test Loss\", model_2_test)\n",
        "print(\"===================\")\n",
        "print(\"Model #1 Test Loss\", model_3_test)"
      ],
      "metadata": {
        "id": "ckPdrlFxa9wk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}